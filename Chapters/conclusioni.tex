\myChapter{Conclusioni e sviluppi}
In questa Tesi abbiamo studiato l'iniezione di adversarial attacks su modelli di guida autonoma basati su reti neurali convoluzionali (CNN) \cite{art2018}. 
In particolare abbiamo utilizzato la libreria python Adversarial Robustness Toolbox (ART) \cite{art2018}. Questa libreria fornisce vari attacchi per studiare la robustezza di un
modello decisionale. Abbiamo studiato gli attacchi presenti nell'ART  e individuato quali di essi fosse interessante applicare a un modello.\\

Il  modello  scelto per l'iniezione è stato LearningByCheating (LBC) \cite{lbc}. LBC è un modello di guida autonoma che gira sul simulatore 
Carla \cite{carla}. La sperimentazione si è divisa in due fasi:\begin{itemize}
    \item nella prima abbiamo analizzato il codice di LBC per trovare il punto  in cui iniettare gli attacchi scelti;
    \item nella seconda abbiamo fatto "guidare" LBC su dei percorsi prestabiliti e raccolto i risultati.
\end{itemize} 

Per valutare gli attacchi, abbiamo fatto guidare LBC su 12 percorsi, prima senza nessun attacco iniettato e poi iniettando ciascuno degli attacchi scelti.

I dati raccolti hanno confermato cio che ci aspettavamo, mostrando la vulnerabilità dei CNN agli adversarial attacks. Nelle run con gli attacchi iniettati è stato notato un forte calo nella stabilità della guida e un aumento delle collisioni.
Alcuni attacchi si sono rilevati più efficaci di altri, ma tutti hanno causato il fallimento di alcuni dei percorsi.

I risultati rafforzano l'idea che la security debba essere un aspetto centrale nello sviluppo 
dei veicoli a guida autonoma. Delle macchine vulnerabili potrebbero perdere facilmente il controllo della guida se prese di mira con gli attacchi descritti. Un hacker potrebbe ottenere il controllo
del canale di comunicazione fra sensori e sistema interno per iniettare un attacco. Ovviamente  un exploit del genere non è immediato da sviluppare ma è sicuramente fattibile, costituendo quindi una grave minaccia
per l'affidabilità.

Il lavoro svolto si presta a svariati sviluppi e approfondimenti, due  dei quali sono:\begin{itemize}
    \item lo studio e lo sviluppo di difese dagli adversarial attacks. La libreria ART fornisce anche l'implementazione di metodi per rendere più robuste le CNN.
    Queste difese potrebbero essere inserite all'interno di un modello e valutate per la loro efficacia;
    \item studio e sviluppo di rilevatori di input modificati. Gli adversarial attacks modificano gli input in modo da essere impercettibili ad occhio umano. Una 
    soluzione a questo problema potrebbe essere la creazione di un rilevatore che invece riesca a individuare eventuali modifiche. Anche in questo caso
    la libreria ART  fornisce strumenti adatti per lo scopo.
\end{itemize}