\chapter{L'Adversarial Robustness Toolbox}
L'Adversarial Robustness ToolBox (ART)  è una libreria python che permette  lo  sviluppo di difese per i modelli ad apprendimento automatico, rendendoli più sicuri ed affidabili. Questi modelli sono infatti vulnerabili ai cosidetti "esempi antagonisti": dati in input(immagini, testo ecc.) creati specificatamente per produrre una determinata risposta dal modello. L'ART include questi attacchi e fornisce gli strumenti per sviluppare sistemi di difesa contro di essi. In questa sezione ci concentreremo sugli attacchi, descrivendone il funzionamento e una possibile implementazione su modelli ADAS realizzati all'interno del simulatore Carla.
\\

\noindent Gli attacchi presenti nella libreria sono suddivisi nel seguente modo:
\begin{itemize}
\item Evasion Attacks,dove i dati in input vengono modificati fino ad avere un errore di classificazione

\item Poisoning Attacks. In questo caso l'obiettivo è iniettare  dati  costruiti in modo specifico per compromettere la fase di apprendimento. 
Sono particolarmente efficaci nei casi in cui il riaddestramento è frequente.

\item Extraction Attacks. Nei casi in cui il modello di apprendimento non sia direttamente accessibile, questi tipi di attacchi vengono sviluppati per addestrare un modello sostituto che sia funzionalmente equivalente al modello target.
\end{itemize}

\noindent Nel contesto della guida autonoma, ha senso concentrarsi sugli Evasion Attacks.	 Possiamo infatti assumere che il riaddestramento non avvenga con una tale frequenza da giustificare lo sviluppo intensivo di un attacco poisoning.  Per quanto riguarda gli \textbf{Extraction Attacks} 
, muovendoci in un contesto open source e accademico dove i modelli sono liberamente accessibili, hanno un'utilità limitata per i nostri obiettivi.
Passiamo ora a descrivere alcuni degli esempi presenti nel toolbox.

\section{Trasformazioni Spaziali}
I primi attacchi che consideriamo sono quelli che applicano semplici trasformazioni spaziali alle immagini senza modificare i pixel in modo diretto. Si tratta di attacchi molto interessanti in quanto facilmente implementabili. Si tratta infatti  di ruotare, spostare o sovrapporre più immagini.

\begin{tabular}{|p{3cm}||p{3cm}|p{3cm}|p{3cm}|}

 \hline
 \multicolumn{4}{|c|}{Lista di Attacchi} \\
 \hline
 Nome  & Descrizione attacco & Applicabilità &Implementazione in Carla\\
 \hline
 Adversarial Patch   \cite{patch}
 & L'attacco consiste nell'attaccare su un qualsiasi oggetto  una specifica patch. Questa patch  è creata in modo da  causare errori di classificazione. Può prendere qualsiasi forma  e viene addestrata su un'insieme di immagini, nelle quali verrà applicata con dimensioni e rotazione casuali 
 
 & Un attaccante potrebbe semplicemente stampare la patch e attaccarla ad esempio su un cartello stradale, mandando in confusione i sistemi ADAS. Se si vuole utilizzare l'attacco su delle immagini si può semplicemente porre la patch direttamente sull'immagine.

 & È necessario modificare gli oggetti della simulazione(segnali, veicoli). Nello specifico, dobbiamo intervenire sul motore UE4, responsabile della costruzione di tali oggetti   \\
 \hline
Spatial Transformation Attack \cite{spatial-attack}
 
 & Le immagini che arrivano al classificatore vengono sottoposte a rotazione e traslazione fino a causare un errore di classificazione

& L’attacco potrebbe essere iniettato a livello dei sensori di un sistema ADAS, in modo da modificare direttamente i dati in input causando problemi difficilmente rintracciabili. Questo richiede un tampering della telecamera, per imporre la trasformazione voluta alle immagini.

 & Si utilizzano le API fornite dal simulatore per acquisire le immagini create\\


 \hline
 
\end{tabular}
\section{Perturbazione dell'immagine}
In questo caso l'immagine in input viene sottoposta a una perturbazione, ovvero una modifica di un certo numero di pixel in modo da non essere visibile
 all’occhio umano ma in grado di causare misclassificazione. Questi attacchi risultano essere molto simili tra loro, ma spesso una piccola modifica può causare risultati estremamente diversi. Nello specifico l'adversarial sample $\textbf{x'}$ viene definito come:
 
 \begin{gather*}
 \noindent 	 \textbf{x'} = \textbf{x} + \epsilon_{x}\\
  \{ \textbf{x'} \in \mathbb{R}^{m \times n \times 3} |  \argmax_{j} f(x')\neq \argmax_{i} f(x)\}
 \end{gather*}
 dove $\epsilon_{x} \in \mathbb{R}^{m \times n \times 3}$ è la perturbazione aggiunta all'input \cite{thresh-lowpix}.\\\\

 
 

\noindent\begin{tabular}{|p{3cm}||p{3cm}|p{3cm}|p{3cm}|}
 \hline
 \multicolumn{4}{|c|}{Lista di Attacchi} \\
 \hline
 Nome     & Descrizione attacco & Applicabilità &Implementazione in Carla\\
 \hline
 Threshold Attack \cite{thresh-lowpix}
 
 &  Viene imposta una soglia massima $th$ alla perturbazione. Nello specifico l'attacco ottimizza  il vincolo $ \norm{\epsilon_{x}}_{\infty} 	\leq th$ dove $th$ è uno dei seguenti valori \{1, 3, 5, 10\}
   
 
 & Dobbiamo poter perturbare le immagini e successivamente passarle al classificatore.  Per questo l'attacco verrà implementato a livello dell'unità di elaborazione.
 
 &Si utilizzano le python API fornite dal simulatore per avere accesso alla struttura interna dei veicoli, dove risiede il classificatore\\

 
 \hline
 
 Low Pixel Attack \cite{thresh-lowpix}
 &
 È una variazione del threshold attack usando la norma 0 al posto della norma infinito.
 
 & Poichè l'attacco è concettualmente identico al threshold attack anch'esso verra iniettato nell'unità di elaborazione
 
 &Si accede alla struttura interna dei veicoli con le python API\\
 \hline
 Decision Tree Attack \cite{decTree}
 
 &  Si sfrutta la struttura ad albero del classificatore,  cercando  un cammino dalla foglia originale ad una  foglia che corrisponde a una classe diversa.   Infine si applica la perturbazione all'esempio

& L'attacco viene iniettato all'interno dell'unità di elaborazione del sistema di guida autonoma.

& Si accede alla struttura interna dei veicoli con le python API \\
  \hline
\end{tabular}
